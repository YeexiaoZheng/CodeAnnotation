{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Codes = [\n",
    "    # utils/common\n",
    "    '''\n",
    "    def read_data(filename):\n",
    "        examples = []\n",
    "        with open(filename, encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(tqdm(f.readlines(), desc='----- [Reading]')):\n",
    "                line = line.strip()\n",
    "                js = json.loads(line)\n",
    "                if 'idx' not in js: js['idx'] = idx\n",
    "                code=' '.join(js['code_tokens']).replace('\\n', ' ')\n",
    "                code=' '.join(code.strip().split())\n",
    "                nl=' '.join(js['docstring_tokens']).replace('\\n', '')\n",
    "                nl=' '.join(nl.strip().split())\n",
    "                examples.append(\n",
    "                    Example(idx=idx, source=code, target=nl) \n",
    "                )\n",
    "\n",
    "        return examples\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def write_to_file(output_dir, lines, filename):\n",
    "        if not os.path.exists(output_dir): os.makedirs(output_dir)      # 没有文件夹则创建\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            for idx, line in enumerate(lines):\n",
    "                f.write(str(idx) + '\\t' + line + '\\n')\n",
    "            f.close()\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def save_checkpoint(model, output_dir, desc):\n",
    "        output_desc_dir = os.path.join(output_dir, desc)\n",
    "        if not os.path.exists(output_desc_dir): os.makedirs(output_desc_dir)    # 没有文件夹则创建\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model     # Only save the model it-self\n",
    "        output_model_file = os.path.join(output_desc_dir, \"pytorch_model.bin\")\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    '''\n",
    "    ,\n",
    "    # Processor\n",
    "    '''\n",
    "    def __call__(self, examples, params, stage=None):\n",
    "        features = self.encode(examples, stage)\n",
    "        dataset = self.to_dataset(features)\n",
    "        return self.to_dataloader(dataset, params)\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def encode(self, examples, stage=None):\n",
    "        features = []\n",
    "        for example_index, example in enumerate(tqdm(examples, desc='----- [Encoding]')):\n",
    "            # source\n",
    "            source_ids = torch.LongTensor(self.tokenizer.encode(example.source, \n",
    "                add_special_tokens=True, max_length=self.config.max_source_length, truncation=True))\n",
    "            source_mask = torch.ones_like(source_ids)\n",
    "    \n",
    "            # target\n",
    "            target = 'None' if stage == 'test' else example.target           \n",
    "            target_ids = torch.LongTensor(self.tokenizer.encode(target, \n",
    "                add_special_tokens=True, max_length=self.config.max_target_length, truncation=True))\n",
    "            target_mask = torch.ones_like(target_ids)\n",
    "        \n",
    "            features.append(\n",
    "                InputFeatures(example_index, source_ids, source_mask, target_ids, target_mask)\n",
    "            )\n",
    "        return features\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def decode_one(self, pred):\n",
    "        return self.tokenizer.decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def decode(self, preds):\n",
    "        return [self.decode_one(pred) for pred in tqdm(preds, desc='----- [Decoding]')]\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def to_dataset(self, features: InputFeatures):\n",
    "        all_source_ids = pad_sequence([f.source_ids for f in features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        all_source_mask = pad_sequence([f.source_mask for f in features], batch_first=True, padding_value=0)\n",
    "        all_target_ids = pad_sequence([f.target_ids for f in features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        all_target_mask = pad_sequence([f.target_mask for f in features], batch_first=True, padding_value=0)\n",
    "        return TensorDataset(all_source_ids, all_source_mask, all_target_ids, all_target_mask)\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def to_dataloader(self, dataset, params):\n",
    "        return DataLoader(dataset, **params)\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def metric(self, trues, preds, desc):\n",
    "        write_to_file(self.config.output_dir, trues, (desc + '.gold'))\n",
    "        write_to_file(self.config.output_dir, preds, (desc + '.output'))\n",
    "        predictions = [str(idx) + '\\t' + line for idx, line in enumerate(preds)]\n",
    "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(self.config.output_dir, (desc + '.gold')))\n",
    "        return round(bleu.bleuFromMaps(goldMap, predictionMap)[0], 2)\n",
    "    '''\n",
    "    ,\n",
    "    # Trainer\n",
    "    '''\n",
    "    def train(self, train_loader: DataLoader):\n",
    "        print('[Training info] Num examples: {}, Batch size: {}, Batch: {}'\n",
    "        .format(len(train_loader.dataset), train_loader.batch_size, len(train_loader)))\n",
    "\n",
    "        self.model.train()\n",
    "        loss_list = []\n",
    "        for batch in tqdm(train_loader, desc='----- [Training]'):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            source_ids, source_mask, target_ids, target_mask = batch\n",
    "\n",
    "            if self.config.model_type.lower() == 'codet5':\n",
    "                loss = self.model(input_ids=source_ids, attention_mask=source_mask.gt(0), \n",
    "                                  labels=target_ids, decoder_attention_mask=target_mask.gt(0)).loss\n",
    "            else:\n",
    "                loss, _, _  = self.model(source_ids=source_ids, source_mask=source_mask, \n",
    "                                         target_ids=target_ids, target_mask=target_mask)\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "    \n",
    "        # Loss of train dataset\n",
    "        train_loss = round(sum(loss_list) / len(loss_list), 5)\n",
    "\n",
    "        return train_loss, loss_list\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def valid(self, val_loader: DataLoader):\n",
    "        print('[Validing info] Num examples: {}, Batch size: {}, Batch: {}'\n",
    "        .format(len(val_loader.dataset), val_loader.batch_size, len(val_loader)))\n",
    "\n",
    "        self.model.eval()\n",
    "        eval_loss, tokens_num = 0, 0\n",
    "        true_ids, pred_ids = [], []\n",
    "        for batch in tqdm(val_loader, desc='----- [Validing]'):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            source_ids, source_mask, target_ids, target_mask = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.config.model_type.lower() == 'codet5':\n",
    "                    loss = self.model(input_ids=source_ids, attention_mask=source_mask, \n",
    "                                    labels=target_ids, decoder_attention_mask=target_mask).loss\n",
    "                    eval_loss += loss.item()\n",
    "                    tokens_num += 1\n",
    "                    preds = self.model.generate(source_ids, attention_mask=source_mask, use_cache=True, \n",
    "                                        num_beams=self.config.beam_size, early_stopping=True, max_length=128)\n",
    "                else:\n",
    "                    _, loss, num = self.model(source_ids=source_ids,source_mask=source_mask,\n",
    "                                          target_ids=target_ids,target_mask=target_mask)\n",
    "                    eval_loss += loss.sum().item()\n",
    "                    tokens_num += num.sum().item()\n",
    "                    preds = self.model(source_ids=source_ids, source_mask=source_mask)[:, 0]\n",
    "                    self.processor.decode(preds)\n",
    "                \n",
    "                true_ids.extend(target_ids)\n",
    "                pred_ids.extend(preds)\n",
    "\n",
    "        # Metrics(ppl bleu) of dev dataset    \n",
    "        eval_ppl = round(np.exp(eval_loss / tokens_num), 5)\n",
    "        eval_bleu = self.processor.metric(self.processor.decode(true_ids), self.processor.decode(pred_ids), 'dev')\n",
    "\n",
    "        return eval_ppl, eval_bleu\n",
    "    '''\n",
    "    ,\n",
    "    '''\n",
    "    def predict(self, test_loader: DataLoader):\n",
    "        print('Predicting info] Num examples: {}, Batch size: {}, Batch: {}'\n",
    "        .format(len(test_loader.dataset), test_loader.batch_size, len(test_loader)))\n",
    "\n",
    "        self.model.eval()\n",
    "        pred_ids = []\n",
    "        for batch in tqdm(test_loader, desc='----- [Predicting]'):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            source_ids, source_mask, _, _ = batch\n",
    "            with torch.no_grad():\n",
    "                if self.config.model_type.lower() == 'codet5':\n",
    "                    preds = self.model.generate(source_ids, attention_mask=source_mask, use_cache=True, \n",
    "                            num_beams=self.config.beam_size, early_stopping=True, max_length=128)\n",
    "                else:\n",
    "                    preds = self.model(source_ids=source_ids, source_mask=source_mask)[:, 0]\n",
    "                \n",
    "                pred_ids.extend(preds)\n",
    "\n",
    "        return pred_ids\n",
    "    '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在colab上取消注释这两行\n",
    "# !pip install transformers\n",
    "# !wget https://storage.googleapis.com/sfr-codet5-data-research/finetuned_models/summarize_python_codet5_base.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "    model.load_state_dict(torch.load('gs://sfr-codet5-data-research/finetuned_models/summarize_python_codet5_base.bin'))\n",
    "\n",
    "    text_list = Codes\n",
    "\n",
    "    for text in text_list:\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        generated_ids = model.generate(input_ids, use_cache=True, \n",
    "                  num_beams=10, early_stopping=True, max_length=128)\n",
    "        print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "625a9e90e125179d2e4e722dbf246d9ce1d0a9c1468b83ede14555db0f731cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
